{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1210996,"sourceType":"datasetVersion","datasetId":691304}],"dockerImageVersionId":31234,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install pyspark\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-11T08:57:39.514529Z","iopub.execute_input":"2026-01-11T08:57:39.515554Z","iopub.status.idle":"2026-01-11T08:57:43.447589Z","shell.execute_reply.started":"2026-01-11T08:57:39.515505Z","shell.execute_reply":"2026-01-11T08:57:43.446214Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: pyspark in /usr/local/lib/python3.12/dist-packages (3.5.1)\nRequirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.12/dist-packages (from pyspark) (0.10.9.7)\n","output_type":"stream"}],"execution_count":19},{"cell_type":"markdown","source":"Ø¹Ø¨Ø¯ Ø§Ù„Ø±Ø­Ù…Ù† Ø´ÙˆÙ\nØ£ÙˆÙ„ Ø´ÙŠ Ø§Ù†Ø¨Ù‡ Ø¹Ù„ÙŠÙ‡ Ø§Ù„Ø¬Ù„Ø³Ø© Ù„Ù„Ù…Ù†ØµØ© Ù‡Ù†Ø§ ØªØ¨Ø¯Ø£ ØªØ³Ø¬Ù„ Ø¹Ø§Ù„Ù…ÙˆÙ‚Ø¹ Ù„Ù…Ø§ ØªØ´ØºÙ„ Ù†Øª ÙˆÙ„Ù…Ø§ ÙŠÙƒÙˆÙ† ÙÙŠÙ‡ Ø§Ù†ØªØ±Ù†Øª Ù„ÙƒÙ† Ø¨Ø¯ÙˆÙ† Ù†Øª Ù†Ù‚Ø¯Ø± Ù†Ø³ØªØ®Ø¯Ù…Ù‡ ÙƒÙƒØªØ§Ø¨Ø© Ø§ÙƒÙˆØ§Ø¯ Ù„ÙƒÙ† Ù…Ø§ ÙŠÙ†Ø­ÙØ¸Ùˆ ÙØ§Ù†ØªØ¨Ù‡ \n\nÙ‡Ù„Ù‚ÙŠØª Ø¨Ø¯ÙŠ Ø£Ø´Ø±Ø­Ù„Ùƒ Ù„ÙŠØ´ ÙƒØªØ¨Øª ÙƒÙ„ ÙƒÙˆØ¯ Ø¹Ø´Ø§Ù† ØªÙÙ‡Ù… ÙˆÙŠÙ† Ø£Ù†Ø§ ÙˆØ§ØµÙ„ ÙˆØªÙƒÙ…Ù„ Ù…Ø¹ÙŠ ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹\n\n\nØ§Ù„Ø¢Ù† Ù‡Ø°Ø§ Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„ÙŠ ÙÙˆÙ‚\nÙŠØ¹Ù†ÙŠ Ø¨Ù†Ø³ØªØ®Ø¯Ù… Ù‡Ø°Ø§ Ø§Ù„Ø³Ø·Ø± Ø¹Ø´Ø§Ù† Ù†Ø«Ø¨Øª Ø§Ù„Ø¨Ø§ÙŠ Ø³Ø¨Ø§Ø±Ùƒ Ø¹Ù„Ù‰ ÙƒØ§Ø¬Ù„\n \nÙ„Ø£Ù†Ù‡ Ù…Ø§ Ø¨ØªÙ†Ø²Ù„ Ù…ÙƒØªØ¨Ø© Ø§Ù„Ø¨Ø§ÙŠ Ø³Ø¨Ø§Ø±Ùƒ Ø¨Ø´ÙƒÙ„ Ø§ÙØªØ±Ø§Ø¶ÙŠ ÙÙ‡Ø°Ø§ Ø§Ù„ÙƒÙˆØ¯ Ù„Ø§Ø²Ù… Ù„Ù†Ø«Ø¨ØªÙ‡Ø§ ÙØ§Ù„Ø³ÙŠØ´Ù† ØªØ§Ø¹Ù†Ø§ Ø§Ù„ÙŠ Ù‡Ù†Ø´ØªØºÙ„ Ø¹Ù„ÙŠÙ‡ ÙŠØ¯ÙŠÙˆÙŠØ§ ÙƒÙ…Ø§ ÙØ§Ù„ÙƒÙˆØ¯ Ø§Ù„Ø£ÙˆÙ„ Ù„ØªÙ†Ø²ÙŠÙ„ Ø§Ù„Ù…ÙƒØªØ¨Ø© Ø¨Ø´ÙƒÙ„ ÙŠØ¯ÙˆÙŠ\n\nÙˆØ£ÙÙ‡Ù…Ùƒ Ù…ÙƒØªØ¨Ø© Ø§Ù„Ø¨Ø§ÙŠ Ø³Ø¨Ø§Ø±Ùƒ Ù‡ÙŠ Ø§Ù„ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„ÙŠ Ø¨ØªØ®Ù„ÙŠÙ†Ø§ Ù†Ø³ØªØ®Ø¯Ù… Ø³Ø¨Ø§Ø±Ùƒ Ù‡ÙŠ ØªØ´Ø¨Ù‡ Ø¹Ù…Ù„ Ø§Ù„Ø§Ø¨Ø§ØªØ´ÙŠ Ø³Ø¨Ø§Ø±Ùƒ Ø§Ù„ÙŠ Ù…Ø¹Ù†Ø§ Ù„ÙƒÙ† Ù‡Ø°Ù‡ Ø§Ù„Ø§Ø¯Ø§Ø© Ø¨Ù„ØºØ© Ø§Ù„Ø¨Ø§ÙŠØ«ÙˆÙ† ÙˆØ¨Ø¯ÙˆÙ†Ù‡Ø§ Ù…Ø§ Ù†Ù‚Ø¯Ø± Ù†Ø¨Ø¯Ø£ Ø¬Ù„Ø³Ø© Ø§Ù„Ø³Ø¨Ø§Ø±Ùƒ ÙˆÙ„Ø§ Ù†Ø´ØºÙ„ Ø§ÙˆØ§Ù…Ø± Ø³Ø¨Ø§Ø±Ùƒ Ø¨Ø¬ÙŠØ¨Ù„Ù†Ø§ Ø§ÙŠØ±ÙˆØ± ÙÙ‡Ø°Ø§ Ø§Ù„Ø³Ø·Ø± Ø¶Ø±ÙˆØ±ÙŠ ÙŠØ´ØªØºÙ„ Ø¹Ù†Ø¹Ø§ ÙØ§Ù„Ø¬Ù„Ø³Ø©\n\nThe pyspark library must be downloaded before starting the rapid and parallel data analysis process using Spark.\n","metadata":{}},{"cell_type":"code","source":"from pyspark.sql import SparkSession\n\nspark = SparkSession.builder \\\n    .appName(\"Cloud-Based DDoS Analysis Platform\") \\\n    .master(\"local[*]\") \\\n    .getOrCreate()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-11T08:57:43.450169Z","iopub.execute_input":"2026-01-11T08:57:43.450628Z","iopub.status.idle":"2026-01-11T08:57:43.460559Z","shell.execute_reply.started":"2026-01-11T08:57:43.450581Z","shell.execute_reply":"2026-01-11T08:57:43.459521Z"}},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":"Ù‡Ø®ØªØµØ± Ø·Ø±ÙŠÙ‚ ÙˆØ§Ù‚Ù„Ùƒ Ø¹Ø´Ø§Ù† Ø´Ùˆ Ø§Ø¹Ù…Ù„Øª ÙƒÙ„ ÙƒÙˆØ¯ ÙØ¨Ø¹Ø¯ ÙƒÙ„ ÙƒÙˆØ¯ Ù‡ØªØ´ÙˆÙ ÙƒÙ„Ù…Ø© #Ø¹Ø´Ø§Ù† Ø§Ù„ÙŠ Ù‡ÙŠ Ø§Ù†Øª ÙØ§Ù‡Ù…Ù‡Ø§ ÙŠØ¹Ù†ÙŠ Ù†ÙØ³ Ù„Ø³Ø¨Ø¨ ÙˆÙ…Ù† Ù‡Ù†Ø§ Ù‡Ø¨Ø¯Ø£\n\n\nØ¹Ø´Ø§Ù† Ø¨Ø¯Ù†Ø§ Ù†Ø¨Ø¯Ø£ Ø§Ù„Ø¬Ù„Ø³Ø© Ù„Ø§Ø²Ù… Ù†ÙƒØªØ¨ Ù‡Ø°Ø§ Ø§Ù„ÙƒÙˆØ¯ ÙƒÙ…Ø§ ÙÙŠ ØªØ¹Ù„ÙŠÙ…Ø§Øª Ù…ÙˆÙ‚Ø¹ Ø§Ù„Ø³Ø¨Ø§Ø±Ùƒ Ù„Ø¨Ø¯Ø§ÙŠØ© Ø§Ù„Ø³ÙŠØ´Ø´Ù† Ø§Ùˆ Ø§Ù„Ø¬Ù„Ø³Ø© Ù„Ù„Ø³Ø¨Ø§Ø±Ùƒ\nØ§Ù„ÙƒÙˆØ¯ ÙƒÙ„Ù‡ Ø¹Ø¨Ø§Ø±Ø© Ø¹Ù† Ø¨Ù„ÙˆÙƒ Ù…Ø´ Ø¶Ø±ÙˆØ±ÙŠ ØªÙÙ‡Ù…Ù‡ Ù…Ø¹ Ø§Ù†Ù‡ Ù…Ù† Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ù„ÙˆØ­Ø¯Ù‡ ÙŠÙ†ÙÙ‡Ù… Ù…Ø«Ù„ Ø§Ù†Ù‡ Ø§Ù„Ø³Ø¨Ø§Ø±Ùƒ Ø¨Ù†Ù‰ ØªØ·Ø¨ÙŠÙ‚ ÙˆØ³Ù…Ø§Ù‡ Ø¬ÙˆØ§ Ø§Ù„Ø³Ù„Ø§Ø´ Ù†ÙƒØªØ¨ Ø§ÙŠ Ø§Ø³Ù… Ø¨Ø¯Ù†Ø§ Ø§ÙŠØ§Ù‡ ÙˆØ¨Ø¹Ø¯ÙŠÙ‡Ø§ Ø§Ø¹Ø·Ù†Ø§Ù‡ ØµÙ„Ø§Ø­ÙŠØ© Ø§Ù„ÙˆØµÙˆÙ„ Ù„ÙƒÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ø­Ù„ÙŠØ§ ÙˆØ¨Ø¹Ø¯Ù‡Ø§ Ù‚Ù„ØªÙ„Ù‡ ÙŠØ§ Ø¨ØªØ§Ø®Ø¯ Ø§Ù„Ø¬Ù„Ø³Ø© Ù„Ùˆ ÙƒØ§Ù†Øª Ù…Ø¹Ù…ÙˆÙ„Ø© ÙŠØ§ Ø¨ØªÙ†Ø´Ø¦Ù‡Ø§ Ù„Ùˆ Ù„Ø³Ø§ Ù„Ø§ÙˆÙ„ Ù…Ø±Ø© Ù‡ØªØ¹Ù…Ù„Ù‡Ø§\n\nTo start a session, we need to write this code as per the instructions on the Spark Apache website for starting a session. The entire code is a block; you don't necessarily need to understand it, although it's understandable from the words themselves. For example, Spark built an application and named it within the slash. We write any name we want, then we give it permission to access all local data. After that, I tell it to either take the session if it already exists or create it if it's the first time you're doing this.","metadata":{}},{"cell_type":"code","source":"from pyspark.sql.functions import col, isnan, when, count, mean, min, max, stddev\nfrom pyspark.ml.classification import DecisionTreeClassifier\nfrom pyspark.ml.feature import StringIndexer, VectorAssembler\nfrom pyspark.ml.clustering import KMeans\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\n\n# ==========================\n#        Ø¯ÙˆØ§Ù„ Ø§Ù„Ø¥Ø­ØµØ§Ø¡\n# ==========================\ndef dataset_info():\n    global df\n    return f\"Rows: {df.count()} | Columns: {len(df.columns)}\"\n\ndef missing_values():\n    global df\n    rows = df.count()\n    stats = df.select([\n        (count(when(col(c).isNull() | isnan(c), c)) / rows * 100).alias(c)\n        for c in df.columns\n    ])\n    return stats.toPandas().T\n\ndef descriptive_stats():\n    global df\n    numeric_cols = [c for c, t in df.dtypes if t in ('int', 'double', 'float', 'long')]\n    stats = df.select(\n        [mean(c).alias(c+\"_mean\") for c in numeric_cols] +\n        [min(c).alias(c+\"_min\") for c in numeric_cols] +\n        [max(c).alias(c+\"_max\") for c in numeric_cols] +\n        [stddev(c).alias(c+\"_std\") for c in numeric_cols]\n    )\n    return stats.toPandas().T\n\ndef average_numeric_columns():\n    global df\n    numeric_cols = [c for c, t in df.dtypes if t in ('int', 'double', 'float', 'long')]\n    averages = df.select([mean(c).alias(c) for c in numeric_cols])\n    return averages.toPandas().T\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-11T08:57:43.462014Z","iopub.execute_input":"2026-01-11T08:57:43.462391Z","iopub.status.idle":"2026-01-11T08:57:43.480244Z","shell.execute_reply.started":"2026-01-11T08:57:43.462335Z","shell.execute_reply":"2026-01-11T08:57:43.479213Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"from pyspark.ml import Pipeline\nfrom pyspark.ml.feature import StringIndexer, VectorAssembler\nfrom pyspark.ml.classification import DecisionTreeClassifier\nfrom pyspark.ml.regression import LinearRegression\nfrom pyspark.ml.clustering import KMeans\nfrom pyspark.ml.fpm import FPGrowth\n\n# Ø¯Ø§Ù„Ø© ØªØ­ÙˆÙŠÙ„ ÙƒÙ„ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ù†ØµÙŠØ© Ù„Ø±Ù‚Ù…ÙŠØ©\ndef prepare_features(df, target_col=None):\n    string_cols = [c for c, t in df.dtypes if t == \"string\" and c != target_col]\n    numeric_cols = [c for c, t in df.dtypes if t in [\"int\", \"double\", \"float\", \"long\"]]\n\n    indexers = [StringIndexer(inputCol=c, outputCol=f\"{c}_indexed\", handleInvalid=\"skip\") for c in string_cols]\n    feature_cols = numeric_cols + [f\"{c}_indexed\" for c in string_cols]\n\n    assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\", handleInvalid=\"skip\")\n    pipeline = Pipeline(stages=indexers + [assembler])\n\n    df_prepared = pipeline.fit(df).transform(df)\n    return df_prepared, feature_cols\n\n\n\ndef run_decision_tree(df, label_col):\n    if label_col not in df.columns:\n        return f\"Column '{label_col}' not found\"\n\n    feature_cols = [\n        c for c, t in df.dtypes\n        if t in (\"int\", \"double\", \"float\", \"long\") and c != label_col\n    ]\n\n    if len(feature_cols) < 1:\n        return \"Not enough numeric features\"\n\n    indexer = StringIndexer(\n        inputCol=label_col,\n        outputCol=\"label\",\n        handleInvalid=\"skip\"\n    )\n\n    df_indexed = indexer.fit(df).transform(df)\n\n    assembler = VectorAssembler(\n        inputCols=feature_cols,\n        outputCol=\"features\",\n        handleInvalid=\"skip\"\n    )\n\n    df_final = assembler.transform(df_indexed).select(\"features\", \"label\")\n\n    train_df, test_df = df_final.randomSplit([0.7, 0.3], seed=42)\n\n    dt = DecisionTreeClassifier(\n        labelCol=\"label\",\n        featuresCol=\"features\",\n        maxDepth=4\n    )\n\n    model = dt.fit(train_df)\n    preds = model.transform(test_df)\n\n    evaluator = MulticlassClassificationEvaluator(\n        labelCol=\"label\",\n        predictionCol=\"prediction\",\n        metricName=\"accuracy\"\n    )\n\n    accuracy = evaluator.evaluate(preds)\n\n    path = f\"/kaggle/working/decision_tree_{label_col}.csv\"\n    preds.select(\"prediction\", \"label\").toPandas().to_csv(path, index=False)\n\n    return f\"Decision Tree Accuracy ({label_col}): {accuracy:.4f}\\n Saved to {path}\"\n\n\n\n\n# -------------------------\n# Linear Regression\n# -------------------------\nfrom pyspark.ml.regression import LinearRegression\nfrom pyspark.ml.evaluation import RegressionEvaluator\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.sql.functions import col\n\ndef run_linear_regression(df, target_col):\n    # Ø§Ù„ØªØ£ÙƒØ¯ Ø£Ù† Ø§Ù„Ø¹Ù…ÙˆØ¯ Ù…ÙˆØ¬ÙˆØ¯\n    if target_col not in df.columns:\n        return f\"Column '{target_col}' not found in dataset\"\n\n    # Ø§Ø®ØªÙŠØ§Ø± ÙƒÙ„ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ø¹Ø¯Ø¯ÙŠØ© Ù…Ø§ Ø¹Ø¯Ø§ Ø§Ù„Ø¹Ù…ÙˆØ¯ Ø§Ù„Ù‡Ø¯Ù ÙƒÙ€ features\n    numeric_cols = [c for c, t in df.dtypes if t in ('int', 'double', 'float', 'long') and c != target_col]\n\n    if not numeric_cols:\n        return \"Not enough numeric features for Linear Regression\"\n\n    assembler = VectorAssembler(inputCols=numeric_cols, outputCol=\"features\", handleInvalid=\"skip\")\n    df_features = assembler.transform(df).select(\"features\", col(target_col).alias(\"label\"))\n\n    lr = LinearRegression(featuresCol=\"features\", labelCol=\"label\")\n    model = lr.fit(df_features)\n\n    preds = model.transform(df_features)\n\n    evaluator = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n    rmse = evaluator.evaluate(preds)\n\n    # Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬\n    path = \"/kaggle/working/linear_regression_predictions.csv\"\n    preds.select(\"prediction\", \"label\").toPandas().to_csv(path, index=False)\n\n    return f\"Linear Regression RMSE: {rmse:.4f}\\n Saved to: {path}\"\n\n\n# -------------------------\ndef run_kmeans(df, k=3):\n    # Ù†Ø®ØªØ§Ø± ÙÙ‚Ø· Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ø±Ù‚Ù…ÙŠØ©\n    feature_cols = [\n        c for c, t in df.dtypes\n        if t in ('int', 'double', 'float', 'long')\n    ]\n\n    if len(feature_cols) < 2:\n        return \" Not enough numeric columns for KMeans\"\n\n    # ØªØ¬Ù…ÙŠØ¹ Ø§Ù„Ø®ØµØ§Ø¦Øµ\n    assembler = VectorAssembler(\n        inputCols=feature_cols,\n        outputCol=\"features\",\n        handleInvalid=\"skip\"\n    )\n\n    df_features = assembler.transform(df).select(\"features\")\n\n    # ØªØ¯Ø±ÙŠØ¨ KMeans\n    kmeans = KMeans(\n        k=k,\n        seed=42,\n        featuresCol=\"features\"\n    )\n\n    model = kmeans.fit(df_features)\n    preds = model.transform(df_features)\n\n    # Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬\n    path = \"/kaggle/working/kmeans_clusters.csv\"\n    preds.select(\"prediction\").toPandas().to_csv(path, index=False)\n\n    return (\n        f\" KMeans clustering completed\\n\"\n        f\" Number of clusters: {k}\\n\"\n        f\" Saved to {path}\"\n    )\n\n\n# -------------------------\n# FPGrowth Frequent Itemsets\n# -------------------------\nfrom pyspark.sql.functions import array, col\nfrom pyspark.sql.types import StringType\n\ndef run_fpgrowth(df, item_col):\n    # Ø§Ù„ØªØ£ÙƒØ¯ Ø£Ù† Ø§Ù„Ø¹Ù…ÙˆØ¯ Ù…ÙˆØ¬ÙˆØ¯\n    if item_col not in df.columns:\n        return f\" Column '{item_col}' not found in dataset\"\n\n    # ØªØ­ÙˆÙŠÙ„ Ø£ÙŠ Ù‚ÙŠÙ…Ø© ÙÙŠ Ø§Ù„Ø¹Ù…ÙˆØ¯ Ø¥Ù„Ù‰\n    # array<string>\n    df_items = df.select(array(col(item_col).cast(StringType())).alias(\"items\")).dropna()\n\n    # FPGrowth\n    fp = FPGrowth(\n        itemsCol=\"items\",\n        minSupport=0.02,\n        minConfidence=0.5\n    )\n\n    model = fp.fit(df_items)\n\n    # Ù‡Ù†Ø§ Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬\n    itemsets_path = \"/kaggle/working/fpgrowth_itemsets.csv\"\n    rules_path = \"/kaggle/working/fpgrowth_rules.csv\"\n\n    model.freqItemsets.toPandas().to_csv(itemsets_path, index=False)\n    model.associationRules.toPandas().to_csv(rules_path, index=False)\n\n    return (\n        \"âœ… FPGrowth completed successfully\\n\"\n        f\" Itemsets saved to: {itemsets_path}\\n\"\n        f\" Rules saved to: {rules_path}\"\n    )\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-11T08:57:43.483940Z","iopub.execute_input":"2026-01-11T08:57:43.484308Z","iopub.status.idle":"2026-01-11T08:57:43.510569Z","shell.execute_reply.started":"2026-01-11T08:57:43.484277Z","shell.execute_reply":"2026-01-11T08:57:43.509332Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"import time\n\ndef load_uploaded_dataset():\n    global df\n\n    if not upload.value or len(upload.value) == 0:\n        print(\" Please upload a dataset first\")\n        return None\n\n    file_info = upload.value[0]\n    uploaded_filename = file_info['name']\n    content = file_info['content']\n\n    # Ø±Ø³Ø§Ù„Ø© Ø¨Ø³ÙŠØ·Ø© Ù„Ù„Ù…Ø³ØªØ®Ø¯Ù…\n    print(f\" Uploading file '{uploaded_filename}'... please wait\")\n\n    start_time = time.time()  # Ø¨Ø¯Ø¡ Ø§Ù„ÙˆÙ‚Øª\n\n    temp_path = f\"/kaggle/working/{uploaded_filename}\"\n    with open(temp_path, \"wb\") as f:\n        f.write(content)\n\n    df = spark.read.csv(temp_path, header=True, inferSchema=True)\n    if \"_c0\" in df.columns:\n        df = df.drop(\"_c0\")\n\n    elapsed = time.time() - start_time\n    print(f\" Dataset uploaded and loaded: {df.count()} rows, {len(df.columns)} columns\")\n    print(f\" Upload + load took {elapsed:.2f} seconds\")\n\n    return df\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-11T08:57:43.511908Z","iopub.execute_input":"2026-01-11T08:57:43.512667Z","iopub.status.idle":"2026-01-11T08:57:43.534905Z","shell.execute_reply.started":"2026-01-11T08:57:43.512635Z","shell.execute_reply":"2026-01-11T08:57:43.533673Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"import ipywidgets as widgets\nfrom IPython.display import display, clear_output\nfrom pyspark.sql.functions import col, isnan, when, count, mean, min, max, stddev\nfrom pyspark.ml.feature import StringIndexer, VectorAssembler\nfrom pyspark.ml.classification import DecisionTreeClassifier\nfrom pyspark.ml.clustering import KMeans\nfrom pyspark.ml.fpm import FPGrowth\nfrom pyspark.ml.regression import LinearRegression\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator, RegressionEvaluator\n\n# -------------------------------\n# 1ï¸ Widgets\n# -------------------------------\nuser_id = widgets.Text(description=\"User ID:\")\n\nupload = widgets.FileUpload(\n    accept='.csv',\n    multiple=False,\n    description='Upload Dataset'\n)\n\ntask_type = widgets.Dropdown(\n    options=[\"Descriptive Statistics\", \"Machine Learning\"],\n    description=\"Task:\"\n)\n\noperation = widgets.Dropdown(description=\"Operation:\")\n\n# Dropdown Ù„Ù„Ø¹Ù…ÙˆØ¯ Ø§Ù„Ù‡Ø¯Ù ÙŠØ¸Ù‡Ø± ÙÙ‚Ø· Ø¹Ù†Ø¯ Ø§Ø®ØªÙŠØ§Ø± Ø¹Ù…Ù„ÙŠØ§Øª ML\ntarget_column = widgets.Dropdown(description=\"Target Column:\")\ntarget_column.layout.display = 'none'  # Ù…Ø®ÙÙŠ Ø¨Ø§Ù„Ø¨Ø¯Ø§ÙŠØ©\n\nrun_button = widgets.Button(description=\"Run\", button_style=\"success\")\noutput = widgets.Output()\n\n# -------------------------------\n#  ØªØ­Ø¯ÙŠØ« Ø®ÙŠØ§Ø±Ø§Øª Ø§Ù„Ø¹Ù…Ù„ÙŠØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ù‡Ù…Ø©\n# -------------------------------\ndef update_operations(*args):\n    if task_type.value == \"Descriptive Statistics\":\n        operation.options = [\n            \"Dataset Info\",\n            \"Missing Values\",\n            \"Descriptive Statistics\",\n            \"Average Numeric Columns\"\n        ]\n        target_column.layout.display = 'none'\n    elif task_type.value == \"Machine Learning\":\n        operation.options = [\n            \"Decision Tree Classification\",\n            \"KMeans Clustering\",\n            \"FPGrowth Frequent Itemsets\",\n            \"Linear Regression\"\n        ]\n       \n\ntask_type.observe(update_operations, 'value')\n\n\n\ndef update_target_column(*args):\n    \n    if operation.value not in [\"Decision Tree Classification\", \"Linear Regression\"]:\n        target_column.layout.display = 'none'\n        return\n\n \n    target_column.layout.display = 'block'\n\n    # Ù„Ùˆ Ø§Ù„Ø¯Ø§ØªØ§ Ù„Ø³Ù‡ Ù…Ø´ Ù…Ø±ÙÙˆØ¹Ø©\n    if df is None:\n        target_column.options = []\n        return\n\n    # Decision Tree  Ø£ÙŠ Ø¹Ù…ÙˆØ¯\n    if operation.value == \"Decision Tree Classification\":\n        target_column.options = list(df.columns)\n\n    # Linear Regression  Ø£Ø¹Ù…Ø¯Ø© Ø±Ù‚Ù…ÙŠØ© ÙÙ‚Ø·\n    elif operation.value == \"Linear Regression\":\n        numeric_cols = [\n            c for c, t in df.dtypes\n            if t in ('int', 'double', 'float', 'long')\n        ]\n        target_column.options = numeric_cols\n\noperation.observe(update_target_column, 'value')\n\n\ndisplay(user_id, upload, task_type, operation, target_column, run_button, output)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-11T08:57:43.536147Z","iopub.execute_input":"2026-01-11T08:57:43.536849Z","iopub.status.idle":"2026-01-11T08:57:43.591128Z","shell.execute_reply.started":"2026-01-11T08:57:43.536753Z","shell.execute_reply":"2026-01-11T08:57:43.589881Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Text(value='', description='User ID:')","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4f0bd37fcf3e4186a3ea5a3a81aaaf83"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"FileUpload(value=(), accept='.csv', description='Upload Dataset')","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cae9d1e9dd8842679772fe0c24164e1c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Dropdown(description='Task:', options=('Descriptive Statistics', 'Machine Learning'), value='Descriptive Statiâ€¦","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fbd1bdd0dc904a33b226e1e2c6c6c4d3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Dropdown(description='Operation:', options=(), value=None)","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a2863060e77d4e9fbdd57471f037a753"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Dropdown(description='Target Column:', layout=Layout(display='none'), options=(), value=None)","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b7baa01f374d4f89aea7386533e0769b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Button(button_style='success', description='Run', style=ButtonStyle())","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"56e7a74cbf644f83b176b5e6859a126d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Output()","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7b8cf116587447b080a4cd8bcf76ff6e"}},"metadata":{}}],"execution_count":24},{"cell_type":"code","source":"upload.value","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-11T08:57:43.592650Z","iopub.execute_input":"2026-01-11T08:57:43.593045Z","iopub.status.idle":"2026-01-11T08:57:43.604439Z","shell.execute_reply.started":"2026-01-11T08:57:43.593003Z","shell.execute_reply":"2026-01-11T08:57:43.603280Z"}},"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"()"},"metadata":{}}],"execution_count":25},{"cell_type":"code","source":"import os\nimport pandas as pd\n\nRESULTS_DIR = \"/kaggle/working/results/\"\nos.makedirs(RESULTS_DIR, exist_ok=True)\n\ndef save_result(df_result, operation_name):\n   \n    filename = f\"{RESULTS_DIR}{operation_name.replace(' ', '_')}.csv\"\n    \n    if 'pandas' in str(type(df_result)):\n        df_result.to_csv(filename, index=False)\n    else:\n        # Spark DataFrame\n        df_result.write.mode(\"overwrite\").option(\"header\", True).csv(filename)\n    \n    print(f\" Result of '{operation_name}' saved to {filename}\")\n    return filename\n\ndef run_processing(b):\n    with output:\n        clear_output()\n        \n        if not upload.value:\n            print(\" Please upload a dataset first\")\n            return\n        \n        # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ù„Ù\n        filename = load_uploaded_dataset()\n        print(f\" Dataset uploaded and loaded: {filename}\")\n       \n        \n        #  ØªÙ†ÙÙŠØ° Ø§Ù„Ø¹Ù…Ù„ÙŠØ©\n        if task_type.value == \"Descriptive Statistics\":\n            if operation.value == \"Dataset Info\":\n                result = pd.DataFrame([dataset_info()], columns=[\"Info\"])\n                print(result.iloc[0,0])\n                save_result(result, operation.value)\n            elif operation.value == \"Missing Values\":\n                result = missing_values()\n                display(result)\n                save_result(result, operation.value)\n            elif operation.value == \"Descriptive Statistics\":\n                result = descriptive_stats()\n                display(result)\n                save_result(result, operation.value)\n            elif operation.value == \"Average Numeric Columns\":\n                result = average_numeric_columns()\n                display(result)\n                save_result(result, operation.value)\n        \n        elif task_type.value == \"Machine Learning\":\n            if operation.value == \"Decision Tree Classification\":\n                 print(run_decision_tree(df, target_column.value))\n                # result = run_decision_tree(df)\n                # print(result)\n\n\n            elif operation.value == \"Linear Regression\":\n                result = run_linear_regression(df, target_column.value)\n                print(result)\n\n            elif operation.value == \"KMeans Clustering\":\n                print(run_kmeans(df))\n\n            elif operation.value == \"FPGrowth Frequent Itemsets\":\n                result = run_fpgrowth(df, target_column.value)\n                print(result)\n\n\nrun_button.on_click(run_processing)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-11T08:57:43.605539Z","iopub.execute_input":"2026-01-11T08:57:43.605821Z","iopub.status.idle":"2026-01-11T08:57:43.626150Z","shell.execute_reply.started":"2026-01-11T08:57:43.605796Z","shell.execute_reply":"2026-01-11T08:57:43.624906Z"}},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":"Ø¹Ø´Ø§Ù† Ù†Ø­Ø°Ù Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ù…Ø¹Ø¯ÙˆÙ…Ø© Ø£Ùˆ Ø§Ù„ÙØ§Ø±ØºØ©\n\nI remove the column (_c0) from the dataset.\nAfter inspecting the schema in the previous step, I realized that this column\ndoes not contain meaningful information and most likely represents an index\ngenerated during the CSV loading process.\n\nKeeping such a column would not add any value to the analysis and could\nnegatively affect performance when processing large datasets.\nBy removing it early, I reduce the dataset size and ensure that only relevant\nfeatures are kept for descriptive statistics and machine learning tasks.\n\nAfter this cleaning step, the dataset is better prepared for further analysis,\nand the next step is to verify the updated structure and continue with\ndescriptive statistics.","metadata":{}},{"cell_type":"markdown","source":"Ø¬Ù…Ù„Ø© Ø¨Ø±Ù†Øª Ø¨Ø§ÙŠØ«ÙˆÙ† Ø¹Ø§Ø¯ÙŠØ© ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø¯Ø§Ù„Ø© Ù„ÙŠÙ† Ù„ØªØ­Ø³Ø¨ Ø¹Ø¯Ø¯ Ø§Ù„Ø§Ø¹Ù…Ø¯Ø© ÙˆØ¬ÙˆØ§Ù‡Ø§ Ø§Ù„Ø³Ø¨Ø§Ø±Ùƒ Ø¯ÙˆØª Ù…ØªØºÙŠØ± Ø§Ù„Ø³Ø¨Ø§Ø±Ùƒ Ø§Ù„ÙŠ Ø§Ø³ØªØ®Ø¯Ù…Ù†Ø§Ù‡ ÙÙˆÙ‚ Ø§Ù„ÙŠ ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¯Ø§ØªØ§Ø³ÙŠØª Ù„Ù†Ø¬ÙŠØ¨ Ø§Ù„Ø§Ø¹Ù…Ø¯Ø© Ù…Ù† Ø§Ù„Ø¯Ø§ØªØ§ ÙØ±ÙŠÙ… \nÙˆØªØ­ØªÙ‡ Ø§Ø³ØªØ®Ø¯Ù…Ù†Ø§ Ø¯Ø§Ù„Ø© Ø§Ù„Ø³Ø¨Ø§Ø±Ùƒ Ù„Ø·Ø¨Ø¹ Ø§Ù„Ø³ÙƒÙŠÙ…Ø§ ØªØ§Ø¹Øª Ø§Ù„Ø¯Ø§ØªØ§ ÙØ±ÙŠÙ… Ø²ÙŠ Ø§Ø³Ù…Ø§Ø¡ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© ÙˆÙ†ÙˆØ¹Ù‡Ø§ ÙƒØ³ØªØ±ÙŠÙ†Ø¬ Ø£Ùˆ Ø§Ù†ØªØ¬Ø± Ùˆ Ø§Ø°Ø§ ÙŠÙ‚Ø¨Ù„ Ø§Ù„Ø¹Ù…ÙˆØ¯ Ù‚ÙŠÙ…Ø© Ù†Ù„Ù„ Ø£Ùˆ Ù„Ø§\nÙˆ\nÙ„Ù†ØªØ£ÙƒØ¯ Ù…Ù† Ø¹Ø¯Ø¯ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø£Ù†Ù‡ ØªØºÙŠÙŠØ± Ø¨Ø¹Ø¯ Ø§Ù„ØªØ¹Ø¯ÙŠÙ„ ÙˆØ­Ø°Ù Ø§Ù„Ø¹Ù…ÙˆØ¯ Ø§Ù„Ø°ÙŠ Ù„ÙŠØ³ Ù„Ù‡ Ù‚ÙŠØ© ÙØ¨ØªÙ„Ø§Ø­Ø¸ Ø¹Ø¯Ø¯ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© ØµØ§Ø± Ø¨Ø¯Ù„ 46 Ø¨Ø¯Ù„ 47\n\nÙˆÙ‡Ù†Ø§ Ø´Ø±Ø­ Ù…Ø¬Ø±Ø¯ Ø¯ÙˆÙ† Ø²ÙŠØ§Ø¯Ø© Ù„Ù…Ø§ ÙØ¹Ù„ØªÙ‡ Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø§Ù†Ø¬Ù„ÙŠØ²ÙŠØ© \n\nTo validate data types and ensure compatibility with (Spark MLlib) before performing distributed machine learning tasks .\n\nI check the dataset after removing the (_c0) column.\nFirst, I print the number of columns to confirm that one column has been removed,\nand now the dataset has 46 columns instead of 47.\n\nThen, I print the schema of the DataFrame again to inspect the structure.\nThis helps me verify that all remaining columns are correctly typed\n(numerical columns as integers or doubles, categorical columns as strings)\nand that the dataset is ready for further analysis.\n\nBy doing this, I ensure that no unintended columns remain and that the data\ntypes are correct for Spark MLlib processing. The next step after this is\nto start calculating descriptive statistics and understanding the content\nof each column for our machine learning tasks.\n","metadata":{}},{"cell_type":"markdown","source":"Ù…Ø§ ØªÙ‚Ù„Ù‚ Ø±Ø§Ø­ Ø§Ø¹Ø¯Ù„ Ø§Ù„ÙƒÙˆØ¯ Ùˆ Ø§Ù„ØªØ¹Ù„ÙŠÙ‚Ø§Øª Ø¨Ø³ Ø­Ø¨ÙŠØª Ø§Ø¬Ø±Ø¨Ù‡ ÙˆØ§ØªØ£ÙƒØ¯ Ø§Ø°Ø§ Ø±Ø§Ø­ ÙŠØ´ØªØºÙ„ Ø§Ùˆ ÙˆØ§Ù„Ø­Ù…Ø¯Ù„Ù„Ù‡ Ø§Ø´ØªØºÙ„ ÙƒØ§Ù…Ù„","metadata":{}},{"cell_type":"code","source":"\nimport time\nimport pandas as pd\nfrom pyspark.sql import SparkSession\nfrom pyspark.ml.feature import StringIndexer, VectorAssembler\nfrom pyspark.ml.classification import DecisionTreeClassifier\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\n\nDATASET_PATH = \"/kaggle/input/ddos-botnet-attack-on-iot-devices/DDoSdata.csv\"\n\ndef run_performance_test(num_cores):\n    print(f\"\\n{'='*60}\")\n    print(f\"Running Decision Tree with {num_cores} core(s)\")\n    print(f\"{'='*60}\")\n\n    spark = SparkSession.builder \\\n        .appName(f\"DT_{num_cores}_cores\") \\\n        .master(f\"local[{num_cores}]\") \\\n        .config(\"spark.sql.shuffle.partitions\", num_cores * 2) \\\n        .getOrCreate()\n\n    start_time = time.time()\n\n    # Load dataset\n    df = spark.read.csv(DATASET_PATH, header=True, inferSchema=True)\n\n    print(\"ğŸ“Œ Dataset columns:\")\n    print(df.columns)\n\n\n    POSSIBLE_LABELS = [\"attack\", \"Attack\", \"label\", \"Label\", \"class\", \"Category\"]\n\n    label_col = None\n    for col_name in POSSIBLE_LABELS:\n        if col_name in df.columns:\n            label_col = col_name\n            break\n\n    if label_col is None:\n        raise Exception(\" No valid label column found in dataset\")\n\n    print(f\" Using label column: {label_col}\")\n\n    # Encode label\n    indexer = StringIndexer(\n        inputCol=label_col,\n        outputCol=\"label\",\n        handleInvalid=\"skip\"\n    )\n    df = indexer.fit(df).transform(df)\n\n    # Numeric features only\n    feature_cols = [\n        c for c, t in df.dtypes\n        if t in (\"int\", \"double\", \"float\", \"long\") and c != label_col\n    ]\n\n    assembler = VectorAssembler(\n        inputCols=feature_cols,\n        outputCol=\"features\",\n        handleInvalid=\"skip\"\n    )\n\n    df_final = assembler.transform(df).select(\"features\", \"label\")\n\n    train_df, test_df = df_final.randomSplit([0.7, 0.3], seed=42)\n\n    dt = DecisionTreeClassifier(\n        labelCol=\"label\",\n        featuresCol=\"features\",\n        maxDepth=4\n    )\n\n    model = dt.fit(train_df)\n    predictions = model.transform(test_df)\n    predictions.count()  # force execution\n\n    total_time = time.time() - start_time\n\n    evaluator = MulticlassClassificationEvaluator(\n        labelCol=\"label\",\n        predictionCol=\"prediction\",\n        metricName=\"accuracy\"\n    )\n\n    accuracy = evaluator.evaluate(predictions)\n\n    spark.stop()\n\n    return total_time, accuracy\n\n\ncores_list = [1, 2, 4, 8]\nresults = []\nbaseline_time = None\n\nfor cores in cores_list:\n    exec_time, acc = run_performance_test(cores)\n\n    if baseline_time is None:\n        baseline_time = exec_time\n\n    speedup = baseline_time / exec_time\n    efficiency = speedup / cores\n\n    results.append({\n        \"Cores\": cores,\n        \"Execution Time (sec)\": round(exec_time, 2),\n        \"Speedup\": round(speedup, 2),\n        \"Efficiency\": round(efficiency, 2),\n        \"Accuracy\": round(acc, 4)\n    })\n\ndf_results = pd.DataFrame(results)\ndf_results.to_csv(\"/kaggle/working/performance_results.csv\", index=False)\n\nprint(\"\\n PERFORMANCE TEST COMPLETED SUCCESSFULLY\")\nprint(df_results)\nprint(\"\\n Saved to /kaggle/working/performance_results.csv\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-11T12:53:45.944182Z","iopub.execute_input":"2026-01-11T12:53:45.944400Z","iopub.status.idle":"2026-01-11T13:02:06.397921Z","shell.execute_reply.started":"2026-01-11T12:53:45.944376Z","shell.execute_reply":"2026-01-11T13:02:06.396830Z"}},"outputs":[{"name":"stdout","text":"\n============================================================\nRunning Decision Tree with 1 core(s)\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"Setting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n26/01/11 12:53:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n                                                                                \r","output_type":"stream"},{"name":"stdout","text":"ğŸ“Œ Dataset columns:\n['_c0', 'pkSeqID', 'stime', 'flgs', 'flgs_number', 'proto', 'proto_number', 'saddr', 'sport', 'daddr', 'dport', 'pkts', 'bytes', 'state', 'state_number', 'ltime', 'seq', 'dur', 'mean', 'stddev', 'sum', 'min', 'max', 'spkts', 'dpkts', 'sbytes', 'dbytes', 'rate', 'srate', 'drate', 'TnBPSrcIP', 'TnBPDstIP', 'TnP_PSrcIP', 'TnP_PDstIP', 'TnP_PerProto', 'TnP_Per_Dport', 'AR_P_Proto_P_SrcIP', 'AR_P_Proto_P_DstIP', 'N_IN_Conn_P_DstIP', 'N_IN_Conn_P_SrcIP', 'AR_P_Proto_P_Sport', 'AR_P_Proto_P_Dport', 'Pkts_P_State_P_Protocol_P_DestIP', 'Pkts_P_State_P_Protocol_P_SrcIP', 'attack', 'category', 'subcategory']\nâœ… Using label column: attack\n","output_type":"stream"},{"name":"stderr","text":"26/01/11 12:54:41 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n Header: , pkSeqID, stime, flgs_number, proto_number, pkts, bytes, state_number, ltime, seq, dur, mean, stddev, sum, min, max, spkts, dpkts, sbytes, dbytes, rate, srate, drate, TnBPSrcIP, TnBPDstIP, TnP_PSrcIP, TnP_PDstIP, TnP_PerProto, TnP_Per_Dport, AR_P_Proto_P_SrcIP, AR_P_Proto_P_DstIP, N_IN_Conn_P_DstIP, N_IN_Conn_P_SrcIP, AR_P_Proto_P_Sport, AR_P_Proto_P_Dport, Pkts_P_State_P_Protocol_P_DestIP, Pkts_P_State_P_Protocol_P_SrcIP, attack\n Schema: _c0, pkSeqID, stime, flgs_number, proto_number, pkts, bytes, state_number, ltime, seq, dur, mean, stddev, sum, min, max, spkts, dpkts, sbytes, dbytes, rate, srate, drate, TnBPSrcIP, TnBPDstIP, TnP_PSrcIP, TnP_PDstIP, TnP_PerProto, TnP_Per_Dport, AR_P_Proto_P_SrcIP, AR_P_Proto_P_DstIP, N_IN_Conn_P_DstIP, N_IN_Conn_P_SrcIP, AR_P_Proto_P_Sport, AR_P_Proto_P_Dport, Pkts_P_State_P_Protocol_P_DestIP, Pkts_P_State_P_Protocol_P_SrcIP, attack\nExpected: _c0 but found: \nCSV file: file:///kaggle/input/ddos-botnet-attack-on-iot-devices/DDoSdata.csv\n26/01/11 12:54:48 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n Header: , pkSeqID, stime, flgs_number, proto_number, pkts, bytes, state_number, ltime, seq, dur, mean, stddev, sum, min, max, spkts, dpkts, sbytes, dbytes, rate, srate, drate, TnBPSrcIP, TnBPDstIP, TnP_PSrcIP, TnP_PDstIP, TnP_PerProto, TnP_Per_Dport, AR_P_Proto_P_SrcIP, AR_P_Proto_P_DstIP, N_IN_Conn_P_DstIP, N_IN_Conn_P_SrcIP, AR_P_Proto_P_Sport, AR_P_Proto_P_Dport, Pkts_P_State_P_Protocol_P_DestIP, Pkts_P_State_P_Protocol_P_SrcIP, attack\n Schema: _c0, pkSeqID, stime, flgs_number, proto_number, pkts, bytes, state_number, ltime, seq, dur, mean, stddev, sum, min, max, spkts, dpkts, sbytes, dbytes, rate, srate, drate, TnBPSrcIP, TnBPDstIP, TnP_PSrcIP, TnP_PDstIP, TnP_PerProto, TnP_Per_Dport, AR_P_Proto_P_SrcIP, AR_P_Proto_P_DstIP, N_IN_Conn_P_DstIP, N_IN_Conn_P_SrcIP, AR_P_Proto_P_Sport, AR_P_Proto_P_Dport, Pkts_P_State_P_Protocol_P_DestIP, Pkts_P_State_P_Protocol_P_SrcIP, attack\nExpected: _c0 but found: \nCSV file: file:///kaggle/input/ddos-botnet-attack-on-iot-devices/DDoSdata.csv\n26/01/11 12:55:14 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n Header: , pkSeqID, stime, flgs_number, proto_number, pkts, bytes, state_number, ltime, seq, dur, mean, stddev, sum, min, max, spkts, dpkts, sbytes, dbytes, rate, srate, drate, TnBPSrcIP, TnBPDstIP, TnP_PSrcIP, TnP_PDstIP, TnP_PerProto, TnP_Per_Dport, AR_P_Proto_P_SrcIP, AR_P_Proto_P_DstIP, N_IN_Conn_P_DstIP, N_IN_Conn_P_SrcIP, AR_P_Proto_P_Sport, AR_P_Proto_P_Dport, Pkts_P_State_P_Protocol_P_DestIP, Pkts_P_State_P_Protocol_P_SrcIP, attack\n Schema: _c0, pkSeqID, stime, flgs_number, proto_number, pkts, bytes, state_number, ltime, seq, dur, mean, stddev, sum, min, max, spkts, dpkts, sbytes, dbytes, rate, srate, drate, TnBPSrcIP, TnBPDstIP, TnP_PSrcIP, TnP_PDstIP, TnP_PerProto, TnP_Per_Dport, AR_P_Proto_P_SrcIP, AR_P_Proto_P_DstIP, N_IN_Conn_P_DstIP, N_IN_Conn_P_SrcIP, AR_P_Proto_P_Sport, AR_P_Proto_P_Dport, Pkts_P_State_P_Protocol_P_DestIP, Pkts_P_State_P_Protocol_P_SrcIP, attack\nExpected: _c0 but found: \nCSV file: file:///kaggle/input/ddos-botnet-attack-on-iot-devices/DDoSdata.csv\n26/01/11 12:55:40 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n Header: , pkSeqID, stime, flgs_number, proto_number, pkts, bytes, state_number, ltime, seq, dur, mean, stddev, sum, min, max, spkts, dpkts, sbytes, dbytes, rate, srate, drate, TnBPSrcIP, TnBPDstIP, TnP_PSrcIP, TnP_PDstIP, TnP_PerProto, TnP_Per_Dport, AR_P_Proto_P_SrcIP, AR_P_Proto_P_DstIP, N_IN_Conn_P_DstIP, N_IN_Conn_P_SrcIP, AR_P_Proto_P_Sport, AR_P_Proto_P_Dport, Pkts_P_State_P_Protocol_P_DestIP, Pkts_P_State_P_Protocol_P_SrcIP, attack\n Schema: _c0, pkSeqID, stime, flgs_number, proto_number, pkts, bytes, state_number, ltime, seq, dur, mean, stddev, sum, min, max, spkts, dpkts, sbytes, dbytes, rate, srate, drate, TnBPSrcIP, TnBPDstIP, TnP_PSrcIP, TnP_PDstIP, TnP_PerProto, TnP_Per_Dport, AR_P_Proto_P_SrcIP, AR_P_Proto_P_DstIP, N_IN_Conn_P_DstIP, N_IN_Conn_P_SrcIP, AR_P_Proto_P_Sport, AR_P_Proto_P_Dport, Pkts_P_State_P_Protocol_P_DestIP, Pkts_P_State_P_Protocol_P_SrcIP, attack\nExpected: _c0 but found: \nCSV file: file:///kaggle/input/ddos-botnet-attack-on-iot-devices/DDoSdata.csv\n26/01/11 12:56:04 WARN MemoryStore: Not enough space to cache rdd_39_3 in memory! (computed 42.2 MiB so far)\n26/01/11 12:56:04 WARN BlockManager: Persisting block rdd_39_3 to disk instead.\n26/01/11 12:56:09 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n26/01/11 12:56:10 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n Header: , pkSeqID, stime, flgs_number, proto_number, pkts, bytes, state_number, ltime, seq, dur, mean, stddev, sum, min, max, spkts, dpkts, sbytes, dbytes, rate, srate, drate, TnBPSrcIP, TnBPDstIP, TnP_PSrcIP, TnP_PDstIP, TnP_PerProto, TnP_Per_Dport, AR_P_Proto_P_SrcIP, AR_P_Proto_P_DstIP, N_IN_Conn_P_DstIP, N_IN_Conn_P_SrcIP, AR_P_Proto_P_Sport, AR_P_Proto_P_Dport, Pkts_P_State_P_Protocol_P_DestIP, Pkts_P_State_P_Protocol_P_SrcIP, attack\n Schema: _c0, pkSeqID, stime, flgs_number, proto_number, pkts, bytes, state_number, ltime, seq, dur, mean, stddev, sum, min, max, spkts, dpkts, sbytes, dbytes, rate, srate, drate, TnBPSrcIP, TnBPDstIP, TnP_PSrcIP, TnP_PDstIP, TnP_PerProto, TnP_Per_Dport, AR_P_Proto_P_SrcIP, AR_P_Proto_P_DstIP, N_IN_Conn_P_DstIP, N_IN_Conn_P_SrcIP, AR_P_Proto_P_Sport, AR_P_Proto_P_Dport, Pkts_P_State_P_Protocol_P_DestIP, Pkts_P_State_P_Protocol_P_SrcIP, attack\nExpected: _c0 but found: \nCSV file: file:///kaggle/input/ddos-botnet-attack-on-iot-devices/DDoSdata.csv\n26/01/11 12:56:33 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n Header: , pkSeqID, stime, flgs_number, proto_number, pkts, bytes, state_number, ltime, seq, dur, mean, stddev, sum, min, max, spkts, dpkts, sbytes, dbytes, rate, srate, drate, TnBPSrcIP, TnBPDstIP, TnP_PSrcIP, TnP_PDstIP, TnP_PerProto, TnP_Per_Dport, AR_P_Proto_P_SrcIP, AR_P_Proto_P_DstIP, N_IN_Conn_P_DstIP, N_IN_Conn_P_SrcIP, AR_P_Proto_P_Sport, AR_P_Proto_P_Dport, Pkts_P_State_P_Protocol_P_DestIP, Pkts_P_State_P_Protocol_P_SrcIP, attack\n Schema: _c0, pkSeqID, stime, flgs_number, proto_number, pkts, bytes, state_number, ltime, seq, dur, mean, stddev, sum, min, max, spkts, dpkts, sbytes, dbytes, rate, srate, drate, TnBPSrcIP, TnBPDstIP, TnP_PSrcIP, TnP_PDstIP, TnP_PerProto, TnP_Per_Dport, AR_P_Proto_P_SrcIP, AR_P_Proto_P_DstIP, N_IN_Conn_P_DstIP, N_IN_Conn_P_SrcIP, AR_P_Proto_P_Sport, AR_P_Proto_P_Dport, Pkts_P_State_P_Protocol_P_DestIP, Pkts_P_State_P_Protocol_P_SrcIP, attack\nExpected: _c0 but found: \nCSV file: file:///kaggle/input/ddos-botnet-attack-on-iot-devices/DDoSdata.csv\n                                                                                \r","output_type":"stream"},{"name":"stdout","text":"\n============================================================\nRunning Decision Tree with 2 core(s)\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"name":"stdout","text":"ğŸ“Œ Dataset columns:\n['_c0', 'pkSeqID', 'stime', 'flgs', 'flgs_number', 'proto', 'proto_number', 'saddr', 'sport', 'daddr', 'dport', 'pkts', 'bytes', 'state', 'state_number', 'ltime', 'seq', 'dur', 'mean', 'stddev', 'sum', 'min', 'max', 'spkts', 'dpkts', 'sbytes', 'dbytes', 'rate', 'srate', 'drate', 'TnBPSrcIP', 'TnBPDstIP', 'TnP_PSrcIP', 'TnP_PDstIP', 'TnP_PerProto', 'TnP_Per_Dport', 'AR_P_Proto_P_SrcIP', 'AR_P_Proto_P_DstIP', 'N_IN_Conn_P_DstIP', 'N_IN_Conn_P_SrcIP', 'AR_P_Proto_P_Sport', 'AR_P_Proto_P_Dport', 'Pkts_P_State_P_Protocol_P_DestIP', 'Pkts_P_State_P_Protocol_P_SrcIP', 'attack', 'category', 'subcategory']\nâœ… Using label column: attack\n","output_type":"stream"},{"name":"stderr","text":"26/01/11 12:57:15 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n Header: , pkSeqID, stime, flgs_number, proto_number, pkts, bytes, state_number, ltime, seq, dur, mean, stddev, sum, min, max, spkts, dpkts, sbytes, dbytes, rate, srate, drate, TnBPSrcIP, TnBPDstIP, TnP_PSrcIP, TnP_PDstIP, TnP_PerProto, TnP_Per_Dport, AR_P_Proto_P_SrcIP, AR_P_Proto_P_DstIP, N_IN_Conn_P_DstIP, N_IN_Conn_P_SrcIP, AR_P_Proto_P_Sport, AR_P_Proto_P_Dport, Pkts_P_State_P_Protocol_P_DestIP, Pkts_P_State_P_Protocol_P_SrcIP, attack\n Schema: _c0, pkSeqID, stime, flgs_number, proto_number, pkts, bytes, state_number, ltime, seq, dur, mean, stddev, sum, min, max, spkts, dpkts, sbytes, dbytes, rate, srate, drate, TnBPSrcIP, TnBPDstIP, TnP_PSrcIP, TnP_PDstIP, TnP_PerProto, TnP_Per_Dport, AR_P_Proto_P_SrcIP, AR_P_Proto_P_DstIP, N_IN_Conn_P_DstIP, N_IN_Conn_P_SrcIP, AR_P_Proto_P_Sport, AR_P_Proto_P_Dport, Pkts_P_State_P_Protocol_P_DestIP, Pkts_P_State_P_Protocol_P_SrcIP, attack\nExpected: _c0 but found: \nCSV file: file:///kaggle/input/ddos-botnet-attack-on-iot-devices/DDoSdata.csv\n26/01/11 12:57:21 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n Header: , pkSeqID, stime, flgs_number, proto_number, pkts, bytes, state_number, ltime, seq, dur, mean, stddev, sum, min, max, spkts, dpkts, sbytes, dbytes, rate, srate, drate, TnBPSrcIP, TnBPDstIP, TnP_PSrcIP, TnP_PDstIP, TnP_PerProto, TnP_Per_Dport, AR_P_Proto_P_SrcIP, AR_P_Proto_P_DstIP, N_IN_Conn_P_DstIP, N_IN_Conn_P_SrcIP, AR_P_Proto_P_Sport, AR_P_Proto_P_Dport, Pkts_P_State_P_Protocol_P_DestIP, Pkts_P_State_P_Protocol_P_SrcIP, attack\n Schema: _c0, pkSeqID, stime, flgs_number, proto_number, pkts, bytes, state_number, ltime, seq, dur, mean, stddev, sum, min, max, spkts, dpkts, sbytes, dbytes, rate, srate, drate, TnBPSrcIP, TnBPDstIP, TnP_PSrcIP, TnP_PDstIP, TnP_PerProto, TnP_Per_Dport, AR_P_Proto_P_SrcIP, AR_P_Proto_P_DstIP, N_IN_Conn_P_DstIP, N_IN_Conn_P_SrcIP, AR_P_Proto_P_Sport, AR_P_Proto_P_Dport, Pkts_P_State_P_Protocol_P_DestIP, Pkts_P_State_P_Protocol_P_SrcIP, attack\nExpected: _c0 but found: \nCSV file: file:///kaggle/input/ddos-botnet-attack-on-iot-devices/DDoSdata.csv\n26/01/11 12:57:38 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n Header: , pkSeqID, stime, flgs_number, proto_number, pkts, bytes, state_number, ltime, seq, dur, mean, stddev, sum, min, max, spkts, dpkts, sbytes, dbytes, rate, srate, drate, TnBPSrcIP, TnBPDstIP, TnP_PSrcIP, TnP_PDstIP, TnP_PerProto, TnP_Per_Dport, AR_P_Proto_P_SrcIP, AR_P_Proto_P_DstIP, N_IN_Conn_P_DstIP, N_IN_Conn_P_SrcIP, AR_P_Proto_P_Sport, AR_P_Proto_P_Dport, Pkts_P_State_P_Protocol_P_DestIP, Pkts_P_State_P_Protocol_P_SrcIP, attack\n Schema: _c0, pkSeqID, stime, flgs_number, proto_number, pkts, bytes, state_number, ltime, seq, dur, mean, stddev, sum, min, max, spkts, dpkts, sbytes, dbytes, rate, srate, drate, TnBPSrcIP, TnBPDstIP, TnP_PSrcIP, TnP_PDstIP, TnP_PerProto, TnP_Per_Dport, AR_P_Proto_P_SrcIP, AR_P_Proto_P_DstIP, N_IN_Conn_P_DstIP, N_IN_Conn_P_SrcIP, AR_P_Proto_P_Sport, AR_P_Proto_P_Dport, Pkts_P_State_P_Protocol_P_DestIP, Pkts_P_State_P_Protocol_P_SrcIP, attack\nExpected: _c0 but found: \nCSV file: file:///kaggle/input/ddos-botnet-attack-on-iot-devices/DDoSdata.csv\n26/01/11 12:57:52 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n Header: , pkSeqID, stime, flgs_number, proto_number, pkts, bytes, state_number, ltime, seq, dur, mean, stddev, sum, min, max, spkts, dpkts, sbytes, dbytes, rate, srate, drate, TnBPSrcIP, TnBPDstIP, TnP_PSrcIP, TnP_PDstIP, TnP_PerProto, TnP_Per_Dport, AR_P_Proto_P_SrcIP, AR_P_Proto_P_DstIP, N_IN_Conn_P_DstIP, N_IN_Conn_P_SrcIP, AR_P_Proto_P_Sport, AR_P_Proto_P_Dport, Pkts_P_State_P_Protocol_P_DestIP, Pkts_P_State_P_Protocol_P_SrcIP, attack\n Schema: _c0, pkSeqID, stime, flgs_number, proto_number, pkts, bytes, state_number, ltime, seq, dur, mean, stddev, sum, min, max, spkts, dpkts, sbytes, dbytes, rate, srate, drate, TnBPSrcIP, TnBPDstIP, TnP_PSrcIP, TnP_PDstIP, TnP_PerProto, TnP_Per_Dport, AR_P_Proto_P_SrcIP, AR_P_Proto_P_DstIP, N_IN_Conn_P_DstIP, N_IN_Conn_P_SrcIP, AR_P_Proto_P_Sport, AR_P_Proto_P_Dport, Pkts_P_State_P_Protocol_P_DestIP, Pkts_P_State_P_Protocol_P_SrcIP, attack\nExpected: _c0 but found: \nCSV file: file:///kaggle/input/ddos-botnet-attack-on-iot-devices/DDoSdata.csv\n26/01/11 12:57:58 WARN MemoryStore: Not enough space to cache rdd_39_1 in memory! (computed 42.2 MiB so far)\n26/01/11 12:57:58 WARN BlockManager: Persisting block rdd_39_1 to disk instead.\n26/01/11 12:57:58 WARN MemoryStore: Not enough space to cache rdd_39_0 in memory! (computed 65.8 MiB so far)\n26/01/11 12:57:58 WARN BlockManager: Persisting block rdd_39_0 to disk instead.\n26/01/11 12:58:11 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n Header: , pkSeqID, stime, flgs_number, proto_number, pkts, bytes, state_number, ltime, seq, dur, mean, stddev, sum, min, max, spkts, dpkts, sbytes, dbytes, rate, srate, drate, TnBPSrcIP, TnBPDstIP, TnP_PSrcIP, TnP_PDstIP, TnP_PerProto, TnP_Per_Dport, AR_P_Proto_P_SrcIP, AR_P_Proto_P_DstIP, N_IN_Conn_P_DstIP, N_IN_Conn_P_SrcIP, AR_P_Proto_P_Sport, AR_P_Proto_P_Dport, Pkts_P_State_P_Protocol_P_DestIP, Pkts_P_State_P_Protocol_P_SrcIP, attack\n Schema: _c0, pkSeqID, stime, flgs_number, proto_number, pkts, bytes, state_number, ltime, seq, dur, mean, stddev, sum, min, max, spkts, dpkts, sbytes, dbytes, rate, srate, drate, TnBPSrcIP, TnBPDstIP, TnP_PSrcIP, TnP_PDstIP, TnP_PerProto, TnP_Per_Dport, AR_P_Proto_P_SrcIP, AR_P_Proto_P_DstIP, N_IN_Conn_P_DstIP, N_IN_Conn_P_SrcIP, AR_P_Proto_P_Sport, AR_P_Proto_P_Dport, Pkts_P_State_P_Protocol_P_DestIP, Pkts_P_State_P_Protocol_P_SrcIP, attack\nExpected: _c0 but found: \nCSV file: file:///kaggle/input/ddos-botnet-attack-on-iot-devices/DDoSdata.csv\n26/01/11 12:58:28 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n Header: , pkSeqID, stime, flgs_number, proto_number, pkts, bytes, state_number, ltime, seq, dur, mean, stddev, sum, min, max, spkts, dpkts, sbytes, dbytes, rate, srate, drate, TnBPSrcIP, TnBPDstIP, TnP_PSrcIP, TnP_PDstIP, TnP_PerProto, TnP_Per_Dport, AR_P_Proto_P_SrcIP, AR_P_Proto_P_DstIP, N_IN_Conn_P_DstIP, N_IN_Conn_P_SrcIP, AR_P_Proto_P_Sport, AR_P_Proto_P_Dport, Pkts_P_State_P_Protocol_P_DestIP, Pkts_P_State_P_Protocol_P_SrcIP, attack\n Schema: _c0, pkSeqID, stime, flgs_number, proto_number, pkts, bytes, state_number, ltime, seq, dur, mean, stddev, sum, min, max, spkts, dpkts, sbytes, dbytes, rate, srate, drate, TnBPSrcIP, TnBPDstIP, TnP_PSrcIP, TnP_PDstIP, TnP_PerProto, TnP_Per_Dport, AR_P_Proto_P_SrcIP, AR_P_Proto_P_DstIP, N_IN_Conn_P_DstIP, N_IN_Conn_P_SrcIP, AR_P_Proto_P_Sport, AR_P_Proto_P_Dport, Pkts_P_State_P_Protocol_P_DestIP, Pkts_P_State_P_Protocol_P_SrcIP, attack\nExpected: _c0 but found: \nCSV file: file:///kaggle/input/ddos-botnet-attack-on-iot-devices/DDoSdata.csv\n                                                                                \r","output_type":"stream"},{"name":"stdout","text":"\n============================================================\nRunning Decision Tree with 4 core(s)\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"name":"stdout","text":"ğŸ“Œ Dataset columns:\n['_c0', 'pkSeqID', 'stime', 'flgs', 'flgs_number', 'proto', 'proto_number', 'saddr', 'sport', 'daddr', 'dport', 'pkts', 'bytes', 'state', 'state_number', 'ltime', 'seq', 'dur', 'mean', 'stddev', 'sum', 'min', 'max', 'spkts', 'dpkts', 'sbytes', 'dbytes', 'rate', 'srate', 'drate', 'TnBPSrcIP', 'TnBPDstIP', 'TnP_PSrcIP', 'TnP_PDstIP', 'TnP_PerProto', 'TnP_Per_Dport', 'AR_P_Proto_P_SrcIP', 'AR_P_Proto_P_DstIP', 'N_IN_Conn_P_DstIP', 'N_IN_Conn_P_SrcIP', 'AR_P_Proto_P_Sport', 'AR_P_Proto_P_Dport', 'Pkts_P_State_P_Protocol_P_DestIP', 'Pkts_P_State_P_Protocol_P_SrcIP', 'attack', 'category', 'subcategory']\nâœ… Using label column: attack\n","output_type":"stream"},{"name":"stderr","text":"26/01/11 12:59:00 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n Header: , pkSeqID, stime, flgs_number, proto_number, pkts, bytes, state_number, ltime, seq, dur, mean, stddev, sum, min, max, spkts, dpkts, sbytes, dbytes, rate, srate, drate, TnBPSrcIP, TnBPDstIP, TnP_PSrcIP, TnP_PDstIP, TnP_PerProto, TnP_Per_Dport, AR_P_Proto_P_SrcIP, AR_P_Proto_P_DstIP, N_IN_Conn_P_DstIP, N_IN_Conn_P_SrcIP, AR_P_Proto_P_Sport, AR_P_Proto_P_Dport, Pkts_P_State_P_Protocol_P_DestIP, Pkts_P_State_P_Protocol_P_SrcIP, attack\n Schema: _c0, pkSeqID, stime, flgs_number, proto_number, pkts, bytes, state_number, ltime, seq, dur, mean, stddev, sum, min, max, spkts, dpkts, sbytes, dbytes, rate, srate, drate, TnBPSrcIP, TnBPDstIP, TnP_PSrcIP, TnP_PDstIP, TnP_PerProto, TnP_Per_Dport, AR_P_Proto_P_SrcIP, AR_P_Proto_P_DstIP, N_IN_Conn_P_DstIP, N_IN_Conn_P_SrcIP, AR_P_Proto_P_Sport, AR_P_Proto_P_Dport, Pkts_P_State_P_Protocol_P_DestIP, Pkts_P_State_P_Protocol_P_SrcIP, attack\nExpected: _c0 but found: \nCSV file: file:///kaggle/input/ddos-botnet-attack-on-iot-devices/DDoSdata.csv\n26/01/11 12:59:06 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n Header: , pkSeqID, stime, flgs_number, proto_number, pkts, bytes, state_number, ltime, seq, dur, mean, stddev, sum, min, max, spkts, dpkts, sbytes, dbytes, rate, srate, drate, TnBPSrcIP, TnBPDstIP, TnP_PSrcIP, TnP_PDstIP, TnP_PerProto, TnP_Per_Dport, AR_P_Proto_P_SrcIP, AR_P_Proto_P_DstIP, N_IN_Conn_P_DstIP, N_IN_Conn_P_SrcIP, AR_P_Proto_P_Sport, AR_P_Proto_P_Dport, Pkts_P_State_P_Protocol_P_DestIP, Pkts_P_State_P_Protocol_P_SrcIP, attack\n Schema: _c0, pkSeqID, stime, flgs_number, proto_number, pkts, bytes, state_number, ltime, seq, dur, mean, stddev, sum, min, max, spkts, dpkts, sbytes, dbytes, rate, srate, drate, TnBPSrcIP, TnBPDstIP, TnP_PSrcIP, TnP_PDstIP, TnP_PerProto, TnP_Per_Dport, AR_P_Proto_P_SrcIP, AR_P_Proto_P_DstIP, N_IN_Conn_P_DstIP, N_IN_Conn_P_SrcIP, AR_P_Proto_P_Sport, AR_P_Proto_P_Dport, Pkts_P_State_P_Protocol_P_DestIP, Pkts_P_State_P_Protocol_P_SrcIP, attack\nExpected: _c0 but found: \nCSV file: file:///kaggle/input/ddos-botnet-attack-on-iot-devices/DDoSdata.csv\n26/01/11 12:59:21 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n Header: , pkSeqID, stime, flgs_number, proto_number, pkts, bytes, state_number, ltime, seq, dur, mean, stddev, sum, min, max, spkts, dpkts, sbytes, dbytes, rate, srate, drate, TnBPSrcIP, TnBPDstIP, TnP_PSrcIP, TnP_PDstIP, TnP_PerProto, TnP_Per_Dport, AR_P_Proto_P_SrcIP, AR_P_Proto_P_DstIP, N_IN_Conn_P_DstIP, N_IN_Conn_P_SrcIP, AR_P_Proto_P_Sport, AR_P_Proto_P_Dport, Pkts_P_State_P_Protocol_P_DestIP, Pkts_P_State_P_Protocol_P_SrcIP, attack\n Schema: _c0, pkSeqID, stime, flgs_number, proto_number, pkts, bytes, state_number, ltime, seq, dur, mean, stddev, sum, min, max, spkts, dpkts, sbytes, dbytes, rate, srate, drate, TnBPSrcIP, TnBPDstIP, TnP_PSrcIP, TnP_PDstIP, TnP_PerProto, TnP_Per_Dport, AR_P_Proto_P_SrcIP, AR_P_Proto_P_DstIP, N_IN_Conn_P_DstIP, N_IN_Conn_P_SrcIP, AR_P_Proto_P_Sport, AR_P_Proto_P_Dport, Pkts_P_State_P_Protocol_P_DestIP, Pkts_P_State_P_Protocol_P_SrcIP, attack\nExpected: _c0 but found: \nCSV file: file:///kaggle/input/ddos-botnet-attack-on-iot-devices/DDoSdata.csv\n26/01/11 12:59:36 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n Header: , pkSeqID, stime, flgs_number, proto_number, pkts, bytes, state_number, ltime, seq, dur, mean, stddev, sum, min, max, spkts, dpkts, sbytes, dbytes, rate, srate, drate, TnBPSrcIP, TnBPDstIP, TnP_PSrcIP, TnP_PDstIP, TnP_PerProto, TnP_Per_Dport, AR_P_Proto_P_SrcIP, AR_P_Proto_P_DstIP, N_IN_Conn_P_DstIP, N_IN_Conn_P_SrcIP, AR_P_Proto_P_Sport, AR_P_Proto_P_Dport, Pkts_P_State_P_Protocol_P_DestIP, Pkts_P_State_P_Protocol_P_SrcIP, attack\n Schema: _c0, pkSeqID, stime, flgs_number, proto_number, pkts, bytes, state_number, ltime, seq, dur, mean, stddev, sum, min, max, spkts, dpkts, sbytes, dbytes, rate, srate, drate, TnBPSrcIP, TnBPDstIP, TnP_PSrcIP, TnP_PDstIP, TnP_PerProto, TnP_Per_Dport, AR_P_Proto_P_SrcIP, AR_P_Proto_P_DstIP, N_IN_Conn_P_DstIP, N_IN_Conn_P_SrcIP, AR_P_Proto_P_Sport, AR_P_Proto_P_Dport, Pkts_P_State_P_Protocol_P_DestIP, Pkts_P_State_P_Protocol_P_SrcIP, attack\nExpected: _c0 but found: \nCSV file: file:///kaggle/input/ddos-botnet-attack-on-iot-devices/DDoSdata.csv\n26/01/11 12:59:47 WARN MemoryStore: Not enough space to cache rdd_39_3 in memory! (computed 28.0 MiB so far)\n26/01/11 12:59:47 WARN BlockManager: Persisting block rdd_39_3 to disk instead.\n26/01/11 12:59:47 WARN MemoryStore: Not enough space to cache rdd_39_0 in memory! (computed 28.0 MiB so far)\n26/01/11 12:59:47 WARN BlockManager: Persisting block rdd_39_0 to disk instead.\n26/01/11 12:59:47 WARN MemoryStore: Not enough space to cache rdd_39_2 in memory! (computed 42.2 MiB so far)\n26/01/11 12:59:47 WARN BlockManager: Persisting block rdd_39_2 to disk instead.\n26/01/11 12:59:47 WARN MemoryStore: Not enough space to cache rdd_39_1 in memory! (computed 65.8 MiB so far)\n26/01/11 12:59:47 WARN BlockManager: Persisting block rdd_39_1 to disk instead.\n26/01/11 12:59:53 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n Header: , pkSeqID, stime, flgs_number, proto_number, pkts, bytes, state_number, ltime, seq, dur, mean, stddev, sum, min, max, spkts, dpkts, sbytes, dbytes, rate, srate, drate, TnBPSrcIP, TnBPDstIP, TnP_PSrcIP, TnP_PDstIP, TnP_PerProto, TnP_Per_Dport, AR_P_Proto_P_SrcIP, AR_P_Proto_P_DstIP, N_IN_Conn_P_DstIP, N_IN_Conn_P_SrcIP, AR_P_Proto_P_Sport, AR_P_Proto_P_Dport, Pkts_P_State_P_Protocol_P_DestIP, Pkts_P_State_P_Protocol_P_SrcIP, attack\n Schema: _c0, pkSeqID, stime, flgs_number, proto_number, pkts, bytes, state_number, ltime, seq, dur, mean, stddev, sum, min, max, spkts, dpkts, sbytes, dbytes, rate, srate, drate, TnBPSrcIP, TnBPDstIP, TnP_PSrcIP, TnP_PDstIP, TnP_PerProto, TnP_Per_Dport, AR_P_Proto_P_SrcIP, AR_P_Proto_P_DstIP, N_IN_Conn_P_DstIP, N_IN_Conn_P_SrcIP, AR_P_Proto_P_Sport, AR_P_Proto_P_Dport, Pkts_P_State_P_Protocol_P_DestIP, Pkts_P_State_P_Protocol_P_SrcIP, attack\nExpected: _c0 but found: \nCSV file: file:///kaggle/input/ddos-botnet-attack-on-iot-devices/DDoSdata.csv\n26/01/11 13:00:10 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n Header: , pkSeqID, stime, flgs_number, proto_number, pkts, bytes, state_number, ltime, seq, dur, mean, stddev, sum, min, max, spkts, dpkts, sbytes, dbytes, rate, srate, drate, TnBPSrcIP, TnBPDstIP, TnP_PSrcIP, TnP_PDstIP, TnP_PerProto, TnP_Per_Dport, AR_P_Proto_P_SrcIP, AR_P_Proto_P_DstIP, N_IN_Conn_P_DstIP, N_IN_Conn_P_SrcIP, AR_P_Proto_P_Sport, AR_P_Proto_P_Dport, Pkts_P_State_P_Protocol_P_DestIP, Pkts_P_State_P_Protocol_P_SrcIP, attack\n Schema: _c0, pkSeqID, stime, flgs_number, proto_number, pkts, bytes, state_number, ltime, seq, dur, mean, stddev, sum, min, max, spkts, dpkts, sbytes, dbytes, rate, srate, drate, TnBPSrcIP, TnBPDstIP, TnP_PSrcIP, TnP_PDstIP, TnP_PerProto, TnP_Per_Dport, AR_P_Proto_P_SrcIP, AR_P_Proto_P_DstIP, N_IN_Conn_P_DstIP, N_IN_Conn_P_SrcIP, AR_P_Proto_P_Sport, AR_P_Proto_P_Dport, Pkts_P_State_P_Protocol_P_DestIP, Pkts_P_State_P_Protocol_P_SrcIP, attack\nExpected: _c0 but found: \nCSV file: file:///kaggle/input/ddos-botnet-attack-on-iot-devices/DDoSdata.csv\n                                                                                \r","output_type":"stream"},{"name":"stdout","text":"\n============================================================\nRunning Decision Tree with 8 core(s)\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"name":"stdout","text":"ğŸ“Œ Dataset columns:\n['_c0', 'pkSeqID', 'stime', 'flgs', 'flgs_number', 'proto', 'proto_number', 'saddr', 'sport', 'daddr', 'dport', 'pkts', 'bytes', 'state', 'state_number', 'ltime', 'seq', 'dur', 'mean', 'stddev', 'sum', 'min', 'max', 'spkts', 'dpkts', 'sbytes', 'dbytes', 'rate', 'srate', 'drate', 'TnBPSrcIP', 'TnBPDstIP', 'TnP_PSrcIP', 'TnP_PDstIP', 'TnP_PerProto', 'TnP_Per_Dport', 'AR_P_Proto_P_SrcIP', 'AR_P_Proto_P_DstIP', 'N_IN_Conn_P_DstIP', 'N_IN_Conn_P_SrcIP', 'AR_P_Proto_P_Sport', 'AR_P_Proto_P_Dport', 'Pkts_P_State_P_Protocol_P_DestIP', 'Pkts_P_State_P_Protocol_P_SrcIP', 'attack', 'category', 'subcategory']\nâœ… Using label column: attack\n","output_type":"stream"},{"name":"stderr","text":"26/01/11 13:00:43 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n Header: , pkSeqID, stime, flgs_number, proto_number, pkts, bytes, state_number, ltime, seq, dur, mean, stddev, sum, min, max, spkts, dpkts, sbytes, dbytes, rate, srate, drate, TnBPSrcIP, TnBPDstIP, TnP_PSrcIP, TnP_PDstIP, TnP_PerProto, TnP_Per_Dport, AR_P_Proto_P_SrcIP, AR_P_Proto_P_DstIP, N_IN_Conn_P_DstIP, N_IN_Conn_P_SrcIP, AR_P_Proto_P_Sport, AR_P_Proto_P_Dport, Pkts_P_State_P_Protocol_P_DestIP, Pkts_P_State_P_Protocol_P_SrcIP, attack\n Schema: _c0, pkSeqID, stime, flgs_number, proto_number, pkts, bytes, state_number, ltime, seq, dur, mean, stddev, sum, min, max, spkts, dpkts, sbytes, dbytes, rate, srate, drate, TnBPSrcIP, TnBPDstIP, TnP_PSrcIP, TnP_PDstIP, TnP_PerProto, TnP_Per_Dport, AR_P_Proto_P_SrcIP, AR_P_Proto_P_DstIP, N_IN_Conn_P_DstIP, N_IN_Conn_P_SrcIP, AR_P_Proto_P_Sport, AR_P_Proto_P_Dport, Pkts_P_State_P_Protocol_P_DestIP, Pkts_P_State_P_Protocol_P_SrcIP, attack\nExpected: _c0 but found: \nCSV file: file:///kaggle/input/ddos-botnet-attack-on-iot-devices/DDoSdata.csv\n26/01/11 13:00:46 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n Header: , pkSeqID, stime, flgs_number, proto_number, pkts, bytes, state_number, ltime, seq, dur, mean, stddev, sum, min, max, spkts, dpkts, sbytes, dbytes, rate, srate, drate, TnBPSrcIP, TnBPDstIP, TnP_PSrcIP, TnP_PDstIP, TnP_PerProto, TnP_Per_Dport, AR_P_Proto_P_SrcIP, AR_P_Proto_P_DstIP, N_IN_Conn_P_DstIP, N_IN_Conn_P_SrcIP, AR_P_Proto_P_Sport, AR_P_Proto_P_Dport, Pkts_P_State_P_Protocol_P_DestIP, Pkts_P_State_P_Protocol_P_SrcIP, attack\n Schema: _c0, pkSeqID, stime, flgs_number, proto_number, pkts, bytes, state_number, ltime, seq, dur, mean, stddev, sum, min, max, spkts, dpkts, sbytes, dbytes, rate, srate, drate, TnBPSrcIP, TnBPDstIP, TnP_PSrcIP, TnP_PDstIP, TnP_PerProto, TnP_Per_Dport, AR_P_Proto_P_SrcIP, AR_P_Proto_P_DstIP, N_IN_Conn_P_DstIP, N_IN_Conn_P_SrcIP, AR_P_Proto_P_Sport, AR_P_Proto_P_Dport, Pkts_P_State_P_Protocol_P_DestIP, Pkts_P_State_P_Protocol_P_SrcIP, attack\nExpected: _c0 but found: \nCSV file: file:///kaggle/input/ddos-botnet-attack-on-iot-devices/DDoSdata.csv\n26/01/11 13:01:02 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n Header: , pkSeqID, stime, flgs_number, proto_number, pkts, bytes, state_number, ltime, seq, dur, mean, stddev, sum, min, max, spkts, dpkts, sbytes, dbytes, rate, srate, drate, TnBPSrcIP, TnBPDstIP, TnP_PSrcIP, TnP_PDstIP, TnP_PerProto, TnP_Per_Dport, AR_P_Proto_P_SrcIP, AR_P_Proto_P_DstIP, N_IN_Conn_P_DstIP, N_IN_Conn_P_SrcIP, AR_P_Proto_P_Sport, AR_P_Proto_P_Dport, Pkts_P_State_P_Protocol_P_DestIP, Pkts_P_State_P_Protocol_P_SrcIP, attack\n Schema: _c0, pkSeqID, stime, flgs_number, proto_number, pkts, bytes, state_number, ltime, seq, dur, mean, stddev, sum, min, max, spkts, dpkts, sbytes, dbytes, rate, srate, drate, TnBPSrcIP, TnBPDstIP, TnP_PSrcIP, TnP_PDstIP, TnP_PerProto, TnP_Per_Dport, AR_P_Proto_P_SrcIP, AR_P_Proto_P_DstIP, N_IN_Conn_P_DstIP, N_IN_Conn_P_SrcIP, AR_P_Proto_P_Sport, AR_P_Proto_P_Dport, Pkts_P_State_P_Protocol_P_DestIP, Pkts_P_State_P_Protocol_P_SrcIP, attack\nExpected: _c0 but found: \nCSV file: file:///kaggle/input/ddos-botnet-attack-on-iot-devices/DDoSdata.csv\n26/01/11 13:01:17 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n Header: , pkSeqID, stime, flgs_number, proto_number, pkts, bytes, state_number, ltime, seq, dur, mean, stddev, sum, min, max, spkts, dpkts, sbytes, dbytes, rate, srate, drate, TnBPSrcIP, TnBPDstIP, TnP_PSrcIP, TnP_PDstIP, TnP_PerProto, TnP_Per_Dport, AR_P_Proto_P_SrcIP, AR_P_Proto_P_DstIP, N_IN_Conn_P_DstIP, N_IN_Conn_P_SrcIP, AR_P_Proto_P_Sport, AR_P_Proto_P_Dport, Pkts_P_State_P_Protocol_P_DestIP, Pkts_P_State_P_Protocol_P_SrcIP, attack\n Schema: _c0, pkSeqID, stime, flgs_number, proto_number, pkts, bytes, state_number, ltime, seq, dur, mean, stddev, sum, min, max, spkts, dpkts, sbytes, dbytes, rate, srate, drate, TnBPSrcIP, TnBPDstIP, TnP_PSrcIP, TnP_PDstIP, TnP_PerProto, TnP_Per_Dport, AR_P_Proto_P_SrcIP, AR_P_Proto_P_DstIP, N_IN_Conn_P_DstIP, N_IN_Conn_P_SrcIP, AR_P_Proto_P_Sport, AR_P_Proto_P_Dport, Pkts_P_State_P_Protocol_P_DestIP, Pkts_P_State_P_Protocol_P_SrcIP, attack\nExpected: _c0 but found: \nCSV file: file:///kaggle/input/ddos-botnet-attack-on-iot-devices/DDoSdata.csv\n26/01/11 13:01:31 WARN MemoryStore: Not enough space to cache rdd_39_5 in memory! (computed 28.0 MiB so far)\n26/01/11 13:01:31 WARN BlockManager: Persisting block rdd_39_5 to disk instead.\n26/01/11 13:01:34 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n Header: , pkSeqID, stime, flgs_number, proto_number, pkts, bytes, state_number, ltime, seq, dur, mean, stddev, sum, min, max, spkts, dpkts, sbytes, dbytes, rate, srate, drate, TnBPSrcIP, TnBPDstIP, TnP_PSrcIP, TnP_PDstIP, TnP_PerProto, TnP_Per_Dport, AR_P_Proto_P_SrcIP, AR_P_Proto_P_DstIP, N_IN_Conn_P_DstIP, N_IN_Conn_P_SrcIP, AR_P_Proto_P_Sport, AR_P_Proto_P_Dport, Pkts_P_State_P_Protocol_P_DestIP, Pkts_P_State_P_Protocol_P_SrcIP, attack\n Schema: _c0, pkSeqID, stime, flgs_number, proto_number, pkts, bytes, state_number, ltime, seq, dur, mean, stddev, sum, min, max, spkts, dpkts, sbytes, dbytes, rate, srate, drate, TnBPSrcIP, TnBPDstIP, TnP_PSrcIP, TnP_PDstIP, TnP_PerProto, TnP_Per_Dport, AR_P_Proto_P_SrcIP, AR_P_Proto_P_DstIP, N_IN_Conn_P_DstIP, N_IN_Conn_P_SrcIP, AR_P_Proto_P_Sport, AR_P_Proto_P_Dport, Pkts_P_State_P_Protocol_P_DestIP, Pkts_P_State_P_Protocol_P_SrcIP, attack\nExpected: _c0 but found: \nCSV file: file:///kaggle/input/ddos-botnet-attack-on-iot-devices/DDoSdata.csv\n26/01/11 13:01:49 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n Header: , pkSeqID, stime, flgs_number, proto_number, pkts, bytes, state_number, ltime, seq, dur, mean, stddev, sum, min, max, spkts, dpkts, sbytes, dbytes, rate, srate, drate, TnBPSrcIP, TnBPDstIP, TnP_PSrcIP, TnP_PDstIP, TnP_PerProto, TnP_Per_Dport, AR_P_Proto_P_SrcIP, AR_P_Proto_P_DstIP, N_IN_Conn_P_DstIP, N_IN_Conn_P_SrcIP, AR_P_Proto_P_Sport, AR_P_Proto_P_Dport, Pkts_P_State_P_Protocol_P_DestIP, Pkts_P_State_P_Protocol_P_SrcIP, attack\n Schema: _c0, pkSeqID, stime, flgs_number, proto_number, pkts, bytes, state_number, ltime, seq, dur, mean, stddev, sum, min, max, spkts, dpkts, sbytes, dbytes, rate, srate, drate, TnBPSrcIP, TnBPDstIP, TnP_PSrcIP, TnP_PDstIP, TnP_PerProto, TnP_Per_Dport, AR_P_Proto_P_SrcIP, AR_P_Proto_P_DstIP, N_IN_Conn_P_DstIP, N_IN_Conn_P_SrcIP, AR_P_Proto_P_Sport, AR_P_Proto_P_Dport, Pkts_P_State_P_Protocol_P_DestIP, Pkts_P_State_P_Protocol_P_SrcIP, attack\nExpected: _c0 but found: \nCSV file: file:///kaggle/input/ddos-botnet-attack-on-iot-devices/DDoSdata.csv\n                                                                                \r","output_type":"stream"},{"name":"stdout","text":"\nâœ… PERFORMANCE TEST COMPLETED SUCCESSFULLY\n   Cores  Execution Time (sec)  Speedup  Efficiency  Accuracy\n0      1                156.76     1.00        1.00       1.0\n1      2                 90.14     1.74        0.87       1.0\n2      4                 85.89     1.83        0.46       1.0\n3      8                 81.84     1.92        0.24       1.0\n\nğŸ“ Saved to /kaggle/working/performance_results.csv\n","output_type":"stream"}],"execution_count":1}]}